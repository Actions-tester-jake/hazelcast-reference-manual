== WAN

[blue]*Hazelcast IMDG Enterprise Feature*

=== WAN Replication

There could be cases where you need to synchronize multiple Hazelcast clusters to the same state.
Hazelcast WAN Replication allows you to keep multiple Hazelcast clusters in sync by replicating their
state over WAN environments such as the Internet.

Imagine you have different data centers in New York, London and Tokyo each running
an independent Hazelcast cluster. Every cluster
would be operating at native speed in their own LAN (Local Area Network), but you also want some or all record sets in
these clusters to be replicated to each other:
updates in the Tokyo cluster should also replicate to London and New York and updates
in the New York cluster are to be synchronized to the Tokyo and London clusters.

This section explains how you can replicate the state of your clusters over
Wide Area Network (WAN) through Hazelcast WAN Replication.

NOTE: You can download the white paper **Hazelcast on AWS: Best Practices for Deployment** at
link:https://hazelcast.com/resources/amazon-ec2-deployment-guide/[Hazelcast.com^].

NOTE: To be able to use the REST calls related to WAN Replication mentioned in the following sections,
you need to enable the `WAN` REST endpoint group.
See the <<using-the-rest-endpoint-groups, Using the REST Endpoint Groups section>> for details.

==== Defining WAN Replication

Hazelcast supports two different operation modes of WAN Replication:

* **Active-Passive:** This mode is mostly used for failover scenarios where
you want to replicate an active cluster to one
  or more passive clusters, for the purpose of maintaining a backup.
* **Active-Active:** Every cluster is equal, each cluster replicates to all other clusters.
This is normally used to connect different clients to different clusters for the sake of
the shortest path between client and server.

There are two different ways of defining the WAN replication endpoints:

* Static endpoints
* Discovery SPI

You can use at most one of these when defining a single WAN publisher.

===== Defining WAN Replication Using Static Endpoints

Below is an example of declarative configuration of WAN Replication from
New York cluster to target the London cluster:

[source,xml,options="nowrap"]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster-batch">
        <batch-publisher>
            <cluster-name>london</cluster-name>
            <queue-full-behavior>THROW_EXCEPTION</queue-full-behavior>
            <queue-capacity>1000</queue-capacity>
            <batch-size>500</batch-size>
            <batch-max-delay-millis>1000</batch-max-delay-millis>
            <snapshot-enabled>false</snapshot-enabled>
            <response-timeout-millis>60000</response-timeout-millis>
            <acknowledge-type>ACK_ON_OPERATION_COMPLETE</acknowledge-type>
            <target-endpoints>10.3.5.1:5701, 10.3.5.2:5701</target-endpoints>
            <discovery-period-seconds>20</discovery-period-seconds>
        </batch-publisher>
        <consumer>
            <class-name>com.hazelcast.enterprise.wan.replication.YourWanConsumer</class-name>
            <persist-wan-replicated-data>false</persist-wan-replicated-data>
            <properties>
                <property name="host">192.168.2.66</property>
                <property name="vpn.name">YOUR_VPN_NAME</property>
                <property name="username">admin</property>
                <property name="password">YOUR_PASSWORD</property>
                <property name="queue.name">Q/hz/clusterA</property>
            </properties>
        </consumer>
    </wan-replication>
    ...
</hazelcast>
----

The `wan-replication` configuration element has the following sub-elements
and attributes:

* `name`: Name of your WAN replication scheme. This name is referenced in IMap or ICache
configuration when you add WAN Replication for these data structures (using the element
`wan-replication-ref` in the configuration of IMap or ICache).
* `batch-publisher`: WAN publisher which uses the built-in WAN replication implementation
in the Hazelcast Enterprise Edition. It waits until a batch size is reached or a delay time is passed.
* `custom-publisher`: Custom implementation of a WAN publisher implementing `WanReplicationPublisher`.
* `consumer`: Configuration for processing the WAN events received from a target cluster. You can configure
a certain behavior when processing the incoming WAN events  or even configure your own implementation
for a WAN consumer. A custom WAN consumer allows you to define custom processing logic and is usually
used in combination with a custom WAN publisher. A custom consumer is optional and you may simply
omit defining it which causes the default processing logic to be used.

A single WAN replication scheme can contain multiple `batch-publisher` and `custom-publisher`
WAN replication publishers.

The `batch-publisher` element is used to configure the built-in WAN replication
implementation in the Hazelcast Enterprise Edition.
It defines how to connect to the target cluster and how WAN events are sent to a
specific target endpoint. As mentioned above, just before the configuration example,
the target endpoints can be a different cluster defined by static IPs or discovered
using a cloud discovery mechanism.

The `batch-publisher` configuration element has the following sub-elements:
* `cluster-name`: Sets the cluster name used as an endpoint cluster name for authentication
on the target endpoint. If there is no separate publisher ID element defined, this cluster name
is also used as a WAN publisher ID. This ID is then used for identifying the publisher in a
WAN replication scheme. It is mandatory to set this attribute.
* `publisher-id`: Sets the publisher ID used to identify the publisher in a
WAN replication scheme. Setting this ID may be useful when the `wan-replication` element contains
multiple WAN publishers and the cluster names are not unique for all of the WAN replication
publishers in a single WAN replication scheme. It is optional to set this attribute. If this ID
is not specified, the `cluster-name` is used as a publisher ID.
* `batch-size`: Changes the maximum size of events that are sent to the target cluster
in a single batch. The batch of events is not sent until this size is
reached or enough time has elapsed. Its default value is 500. See the <<batch-size, Batch Size section>>.
* `batch-max-delay-millis`: If the number of events generated does not reach the `batch-size`, they
are sent to the target cluster after a certain amount of time is passed. You can set this time in
milliseconds using this element. Its default value is 1000 milliseconds.
See the <<batch-maximum-delay, Batch Maximum Delay section>>.
* `response-timeout-millis`: After a replication event batch is sent to the target cluster, the source
member waits for a confirmation that says the event has reached the target. If confirmation is not
received for a period of `response-timeout-millis`, the event is resent to the target cluster.
The default value is 60000 milliseconds. See the <<response-timeout, Response Timeout section>>.
* `acknowledge-type`: Acknowledgment type for each target cluster when the events are replicated.
See the <<acknowledgment-types, Acknowledgment Types section>>.
* `snapshot-enabled`: Sets if key-based coalescing is configured for this WAN publisher.
When enabled, only the latest event for a key is sent to target. Its default value is `false`.
* `idle-max-park-ns`: Sets the maximum duration in nanoseconds that the WAN replication thread
is parked if there are no events to replicate.
* `idle-min-park-ns`: Sets the minimum duration in nanoseconds that the WAN replication thread
is parked if there are no events to replicate.
* `max-concurrent-invocations`: Sets the maximum number of WAN event batches being sent to the target
cluster concurrently. Setting this element to anything less than 2 only allows a
single batch of events to be sent to each target endpoint and will
maintain causality of events for a single partition.
+
Setting this element to 2 or higher allows multiple batches
of WAN events to be sent to each target endpoint. Since this allows
reordering or batches due to network conditions, causality and ordering
of events for a single partition is lost and batches for a single
partition are now sent randomly to any available target endpoint.
+
This, however, does present faster WAN replication for certain scenarios
such as replicating immutable, independent map entries which are only
added once and where ordering of when these entries are added is not
necessary.
+
Keep in mind that if you set this element to a value which is less than
the target endpoint count, you will lose performance as not all target
endpoints are used at any point in time to process the WAN event batches.
So, for instance, if you have a target cluster with 3 members (target
endpoints) and you want to use this element, it makes sense to set it
to a value higher than 3. Otherwise, you can simply disable it
by setting it to less than 2 in which case WAN will use the
default replication strategy and adapt to the target endpoint count
while maintaining causality.
* `queue-full-behavior`: Policy to be applied when the WAN Replication event queues are full.
See the <<queue-full-behavior, Queue Full Behavior section>>.
* `queue-capacity`: Size of the queue of events. Its default value is 10000. If you exceed this queue size,
then the oldest, not yet replicated updates might get lost. Therefore, if you have a large rate of
put/update/remove operations, you should increase the queue capacity. See the <<queue-capacity, Queue Capacity section>>.
* `target-endpoints`: IP addresses and ports of the cluster members for which the WAN replication is implemented.
It is enough to specify some of the member IP/ports available in the target cluster, i.e., you donâ€™t need to
provide the IP/ports of all members in there. WAN does not perform the discovery of other members in the target cluster;
it only expects that the IP addresses you provide are available.
* `wan-sync`: Configuration for the WAN sync mechanism.
* `aws`: Configuration for discovery strategy for Amazon EC2 discovery plugin.
* `gcp`: Configuration for discovery strategy for Google cloud platform discovery plugin.
* `azure`: Configuration for discovery strategy for Microsoft Azure discovery plugin.
* `kubernetes`: Configuration for discovery strategy for Kubernetes discovery plugin.
* `eureka`: Configuration for discovery strategy for Eureka discovery plugin.
* `discovery-strategies`: Set its `enabled` attribute to true for discovery in various cloud infrastructures.
You can define multiple discovery strategies using the `discovery-strategy` sub-element and its elements.

The `custom-publisher` is used to configure a custom implementation of a WAN replication implementing
`WanReplicationPublisher`. For example, you might implement replication to Kafka or some JMS queue or even
write out map and cache event changes to a log on disk. It has the following sub-elements:

* `class-name`: Mandatory configuration value defining the fully qualified class name of the
WAN publisher implementation.
* `publisher-id`: Mandatory configuration value for the publisher ID used for identifying the
publisher in a `WanReplicationConfig`.

The `consumer` is used to configure the processing of the WAN events received from a source cluster.
You can configure a certain behavior when processing the incoming WAN events or
even configure your own implementation for a WAN consumer. A custom WAN consumer allows you to
define a custom processing logic and is usually used in combination with a custom WAN publisher.
A custom consumer is optional and you may simply omit defining it which causes
the default processing logic to be used. It has the following sub-elements:

* `class-name`: Name of the class implementing a custom WAN consumer (`WanReplicationConsumer`).
If you don't define a class name, the default processing logic for incoming WAN events is used.
* `properties`: Properties for the custom WAN consumer.
These properties are accessible when initializing the WAN consumer.
You can define the host, username and password for the host, name of the queue to be polled by the consumer, etc.
* `persist-wan-replicated-data`: When set to `true`, an incoming event over WAN replication can be
persisted to a database for example, otherwise it is not persisted. Default value is `true`.

And the following is the equivalent programmatic configuration snippet:

[source,java]
----
include::{javasource}/wan/ExampleWANReplicationConfiguration.java[tag=wrc]
----

Using this configuration, the cluster replicates to a cluster with the name "london".
The "london" cluster should have a similar configuration if you want to run in Active-Active mode.

You can achieve various WAN topologies using different configurations on different clusters. For instance,
if the New York and London cluster configurations contain the `wan-replication` element and the Tokyo cluster
does not, it might mean that the New York and London clusters are active endpoints and Tokyo is a passive endpoint.

===== Defining WAN Replication Using Discovery SPI

In addition to defining target cluster endpoints with static IP addresses,
you can configure WAN to work with the discovery SPI and determine the endpoint IP addresses at runtime.
This allows you to use WAN with endpoints on various cloud infrastructures (such as Amazon EC2 or GCP Compute) where
the IP address is not known in advance. Typically you use a readily available discovery SPI plugin such as
link:https://github.com/hazelcast/hazelcast-aws[Hazelcast AWS EC2 discovery plugin^],
link:https://github.com/hazelcast/hazelcast-gcp[Hazelcast GCP discovery plugin^], or similar.
For more advanced cases, you can provide your own discovery SPI implementation with custom logic for
determining the WAN target endpoints such as looking up the endpoints in some service registry or even
reading the endpoint addresses from a file.

NOTE: When using the discovery SPI, WAN always connects to the public address of the members returned by
the discovery SPI implementation. This is opposite to the cluster membership mechanism using
the discovery SPI where a member connects to a different member in the same cluster through its private address.
Should you prefer for WAN to use the private address of the discovered member as well,
please use the `use-endpoint-private-address` publisher element (see below).

Following is an example of setting up the WAN replication with the EC2 discovery plugin.
You must have the link:https://github.com/hazelcast/hazelcast-aws[Hazelcast AWS EC2 discovery plugin^] on the classpath.

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster-batch">
        <batch-publisher>
            <cluster-name>london</cluster-name>
            <queue-full-behavior>THROW_EXCEPTION</queue-full-behavior>
            <queue-capacity>1000</queue-capacity>
            <batch-size>500</batch-size>
            <batch-max-delay-millis>1000</batch-max-delay-millis>
            <snapshot-enabled>false</snapshot-enabled>
            <response-timeout-millis>60000</response-timeout-millis>
            <acknowledge-type>ACK_ON_OPERATION_COMPLETE</acknowledge-type>
            <discovery-period-seconds>20</discovery-period-seconds>
            <max-target-endpoints>5</max-target-endpoints>
            <discovery-strategies>
                <discovery-strategy enabled="true" class="com.hazelcast.aws.AwsDiscoveryStrategy">
                    <properties>
                        <property name="access-key">test-access-key</property>
                        <property name="secret-key">test-secret-key</property>
                        <property name="region">test-region</property>
                        <property name="iam-role">test-iam-role</property>
                        <property name="host-header">ec2.test-host-header</property>
                        <property name="security-group-name">test-security-group-name</property>
                        <property name="tag-key">test-tag-key</property>
                        <property name="tag-value">test-tag-value</property>
                        <property name="connection-timeout-seconds">10</property>
                        <property name="hz-port">5701</property>
                    </properties>
                </discovery-strategy>
            </discovery-strategies>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

The `hz-port` property defines the port or the port range on which the target endpoint is running.
The default port range 5701-5708 is used if this property is not defined.
This is needed because the Amazon API which the AWS plugin uses does not provide
the port on which Hazelcast is running, only the IP address. For some other discovery SPI implementations,
this might not be necessary and it might discover the port as well, e.g., by looking up in a service registry.

The other properties are the same as when using the `aws` element. In case of EC2 discovery you can configure
the WAN replication using the `aws` element. You may use either of these, but not both at the same time.

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster-batch">
        <batch-publisher>
            <cluster-name>london</cluster-name>
            <queue-full-behavior>THROW_EXCEPTION</queue-full-behavior>
            <queue-capacity>1000</queue-capacity>
            <batch-size>500</batch-size>
            <batch-max-delay-millis>1000</batch-max-delay-millis>
            <snapshot-enabled>false</snapshot-enabled>
            <response-timeout-millis>60000</response-timeout-millis>
            <acknowledge-type>ACK_ON_OPERATION_COMPLETE</acknowledge-type>
            <discovery-period-seconds>20</discovery-period-seconds>
            <use-endpoint-private-address>false</use-endpoint-private-address>
            <max-target-endpoints>5</max-target-endpoints>
            <aws enabled="true">
                <access-key>my-access-key</access-key>
                <secret-key>my-secret-key</secret-key>
                <region>us-west-1</region>
                <security-group-name>hazelcast-sg</security-group-name>
                <tag-key>type</tag-key>
                <tag-value>hz-members</tag-value>
                <hz-port>5701</hz-port>
            </aws>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

See the <<aws-element, aws element>> and the <<configuring-client-for-aws, Configuring Client for AWS>>
sections for the descriptions of above AWS configuration elements.

The following are the definitions of additional configuration elements:

* `discovery-period-seconds`: Sets the period in seconds in which WAN tries to discover new target
endpoints and reestablish connections to failed endpoints.
* `max-target-endpoints`: Returns the maximum number of endpoints that WAN connects to when
a discovery mechanism is used to define the endpoints. This element has no effect when static endpoint addresses
are defined using `target-endpoints`. Default is `Integer.MAX_VALUE`.
* `use-endpoint-private-address`: Sets whether the WAN connection manager should connect to the
endpoint on the private address returned by the discovery SPI. By default this element is `false`
which means the WAN connection manager always uses the public address.

You can also define the WAN publisher with discovery SPI using the programmatic configuration:

[source,java]
----
include::{javasource}/wan/ExampleWANReplicationDiscoveryConfiguration.java[tag=wrdc]
----

[[wanbatchreplication-implementation]]
==== WAN Batch Publisher Implementation

Hazelcast IMDG Enterprise Edition offers the WAN batch publisher implementation for WAN replication.

As you see in the above configuration examples, this implementation is specified simply by using
the `batch-publisher` element (in the declarative configuration) or the `WanBatchReplicationPublisherConfig`
class (in the programmatic configuration) when defining a WAN replication publisher.

The WAN batch publisher implementation waits until:

* a pre-defined number of replication events are generated, (see the <<batch-size, Batch Size section>>)
* or a pre-defined amount of time is passed (see the <<batch-maximum-delay, Batch Maximum Delay section>>).

NOTE: `WanNoDelayReplication` implementation has been removed. You can still achieve this behavior by
setting the batch size to `1` while configuring your WAN replication.

==== Configuring WAN Replication for IMap and ICache

You can configure WAN replication for Hazelcast's IMap and ICache data structures.
To enable WAN replication for an IMap or ICache instance, you can use the `wan-replication-ref` element.
Each IMap and ICache instance can have a different WAN replication configuration.

**Enabling WAN Replication for IMap:**

Imagine you have different distributed maps, however only one of those maps should be replicated to a target cluster.
To achieve this, configure the map that you want to be replicated by adding the `wan-replication-ref` element in
the map configuration as shown below.

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster">
        ...
    </wan-replication>
    <map name="my-shared-map">
        <wan-replication-ref name="my-wan-cluster">
            <merge-policy>com.hazelcast.spi.merge.PassThroughMergePolicy</merge-policy>
            <republishing-enabled>false</republishing-enabled>
        </wan-replication-ref>
    </map>
    ...
</hazelcast>
----

The following is the equivalent programmatic configuration:

[source,java]
----
include::{javasource}/wan/EnablingWRforMap.java[tag=wrmap]
----

You see that we have `my-shared-map` configured to replicate itself to the cluster targets defined in the earlier
`wan-replication` element.

`wan-replication-ref` has the following elements;

* `name`: Name of `wan-replication` configuration. IMap or ICache instance uses this `wan-replication` configuration.
* `merge-policy`: Resolve conflicts that occur when the target cluster already has the replicated entry key.
* `republishing-enabled`: When enabled, an incoming event to a member is forwarded to target cluster of that member.
Enabling the event republishing is useful in a scenario where cluster A replicates to cluster B and
cluster B replicates to cluster C. You do not need to enable republishing when all your clusters
replicate to each other.

When using Active-Active Replication, multiple clusters can simultaneously update the same
entry in a distributed data structure. You can configure a merge policy to resolve these potential
conflicts, as shown in the above example configuration (using the `merge-policy` sub-element under
the `wan-replication-ref` element).

Hazelcast provides the following merge policies for IMap:

* `com.hazelcast.spi.merge.PutIfAbsentMergePolicy`: Incoming entry merges from the source map to
the target map if it does not exist in the target map.
* `com.hazelcast.spi.merge.HigherHitsMergePolicy`: Incoming entry merges from the source map to
the target map if the source entry has more hits than the target one.
* `com.hazelcast.spi.merge.PassThroughMergePolicy`: Incoming entry merges from the source map to
the target map unless the incoming entry is not null.
* `com.hazelcast.spi.merge.ExpirationTimeMergePolicy`: Incoming entry merges from the source map to
the target map if the source entry will expire later than the destination entry.
Please note that this merge policy can only be used when the clusters' clocks are in sync.
* `com.hazelcast.spi.merge.LatestAccessMergePolicy`: Incoming entry merges from the source map to
the target map if the source entry has been accessed more recently than the destination entry.
Please note that this merge policy can only be used when the clusters' clocks are in sync.
* `com.hazelcast.spi.merge.LatestUpdateMergePolicy`: Incoming entry merges from the source map to
the target map if the source entry has been updated more recently than the target entry.
Please note that this merge policy can only be used when the clusters' clocks are in sync.

NOTE: When using WAN replication, please note that only key based events are replicated to the target cluster.
Operations like `clear`, `destroy`, `evict` and `evictAll` are NOT replicated.

**Enabling WAN Replication for ICache:**

The following is a declarative configuration example for enabling WAN Replication for ICache:

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster">
        ...
    </wan-replication>
    <cache name="my-shared-cache">
        <wan-replication-ref name="my-wan-cluster">
            <merge-policy>com.hazelcast.spi.merge.PassThroughMergePolicy</merge-policy>
            <republishing-enabled>true</republishing-enabled>
        </wan-replication-ref>
    </cache>
    ...
</hazelcast>
----

The following is the equivalent programmatic configuration:

[source,java]
----
include::{javasource}/wan/EnablingWRforCache.java[tag=wrcache]
----

NOTE: Caches that are created dynamically do not support WAN replication functionality.
Cache configurations should be defined either declaratively (by XML) or programmatically on both
source and target clusters.

Hazelcast provides the following merge policies for ICache:

* `com.hazelcast.spi.merge.PutIfAbsentMergePolicy`: Incoming entry merges from the source cache to
the target cache if it does not exist in the target cache.
* `com.hazelcast.spi.merge.HigherHitsMergePolicy`: Incoming entry merges from the source cache to
the target cache if the source entry has more hits than the target one.
* `com.hazelcast.spi.merge.PassThroughMergePolicy`: Incoming entry merges from the source cache to
the target cache unless the incoming entry is not null.
* `com.hazelcast.spi.merge.ExpirationTimeMergePolicy`: Incoming entry merges from the source cache to
the target cache if the source entry will expire later than the destination entry.
Please note that this merge policy can only be used when the clusters' clocks are in sync.
* `com.hazelcast.spi.merge.LatestAccessMergePolicy`: Incoming entry merges from the source cache to
the target cache if the source entry has been accessed more recently than the destination entry.
Please note that this merge policy can only be used when the clusters' clocks are in sync.
* `com.hazelcast.spi.merge.LatestUpdateMergePolicy`: Incoming entry merges from the source cache to
the target cache if the source entry has been updated more recently than the target entry.
Please note that this merge policy can only be used when the clusters' clocks are in sync.

==== Batch Size

The maximum size of events that are sent in a single batch can be changed
depending on your needs. Default value for batch size is `500`.

Batch size can be set for each target cluster by modifying related `WanBatchReplicationPublisherConfig`.

Below is the declarative configuration for changing the value of the element:

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster">
        <batch-publisher>
            <cluster-name>london</cluster-name>
            <batch-size>1000</batch-size>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

And, following is the equivalent programmatic configuration:

[source,java]
----
WanReplicationConfig wanConfig = config.getWanReplicationConfig("my-wan-cluster");
WanBatchReplicationPublisherConfig publisherConfig = new WanBatchReplicationPublisherConfig()
        .setClusterName("london")
        .setBatchSize(1000);
wanConfig.addWanPublisherConfig(publisherConfig);
----

NOTE: `WanNoDelayReplication` implementation has been removed.
You can still achieve this behavior by setting the batch size to `1` while configuring your WAN replication.

==== Batch Maximum Delay

When using the built-in WAN batch replication, if the number of WAN replication events generated does not
reach <<batch-size, Batch Size>>, they are sent to the target cluster after a certain amount of time is passed.
You can set this duration in milliseconds using this batch maximum delay configuration.
Default value of for this duration is 1 second (1000 milliseconds).

Maximum delay can be set for each target cluster by modifying related `WanBatchReplicationPublisherConfig`.

You can change this element using the declarative configuration as shown below.

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster">
        <batch-publisher>
            <cluster-name>london</cluster-name>
            <batch-max-delay-millis>2000</batch-max-delay-millis>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

And, the following is the equivalent programmatic configuration:

[source,java]
----
WanReplicationConfig wanConfig = config.getWanReplicationConfig("my-wan-cluster");
WanBatchReplicationPublisherConfig publisherConfig = new WanBatchReplicationPublisherConfig()
        .setClusterName("london")
        .setBatchMaxDelayMillis(2000);
wanConfig.addWanPublisherConfig(publisherConfig);
----

==== Response Timeout

After a replication event is sent to the target cluster, the source member waits for
an acknowledgement of the delivery of the event to the target.
If the confirmation is not received inside a timeout duration window, the event is resent to
the target cluster. Default value of this duration is `60000` milliseconds.

You can change this duration depending on your network latency for each target cluster by
modifying related `WanBatchReplicationPublisherConfig`.

Below is an example of declarative configuration:

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster">
        <batch-publisher>
            <cluster-name>london</cluster-name>
            <response-timeout-millis>5000</response-timeout-millis>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

And, the following is the equivalent programmatic configuration:

[source,java]
----
WanReplicationConfig wanConfig = config.getWanReplicationConfig("my-wan-cluster");
WanBatchReplicationPublisherConfig publisherConfig = new WanBatchReplicationPublisherConfig()
        .setClusterName("london")
        .setResponseTimeoutMillis(5000);
wanConfig.addWanPublisherConfig(publisherConfig);
----

==== Queue Capacity

For huge clusters or high data mutation rates, you might need to increase the replication queue size. The default queue
size for replication queues is `10000`. This means, if you have heavy put/update/remove rates, you might exceed the queue size
so that the oldest, not yet replicated, updates might get lost.
Note that a separate queue is used for each WAN Replication configured for IMap and ICache.

Queue capacity can be set for each target cluster by modifying related `WanBatchReplicationPublisherConfig`.

You can change this element using the declarative configuration as shown below.

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster">
        <batch-publisher>
            <cluster-name>london</cluster-name>
            <queue-capacity>15000</queue-capacity>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

And, the following is the equivalent programmatic configuration:

[source,java]
----
WanReplicationConfig wanConfig = config.getWanReplicationConfig("my-wan-cluster");
WanBatchReplicationPublisherConfig publisherConfig = new WanBatchReplicationPublisherConfig()
        .setClusterName("london")
        .setQueueCapacity(15000);
wanConfig.addWanPublisherConfig(publisherConfig);
----

Note that you can clear a member's WAN replication event queue.
It can be initiated through Management Center's
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#monitoring-wan-replication[Clear Queues action^] or
Hazelcastâ€™s REST API. Below is the URL for its REST call:

```
http://member_ip:port/hazelcast/rest/wan/clearWanQueues
```

You need to add the following URL-encoded parameters to the request in the following order separated by "&";

* Cluster name
* Cluster password
* Name of the WAN replication configuration
* WAN replication publisher ID/target cluster name

This may be useful, for instance, to release the consumed heap if you know that
the target cluster is being shut down, decommissioned, put out of use and it will never come back.
Or, when a failure happens and queues are not replicated anymore, you could clear the queues using this clearing action.

==== Queue Full Behavior

In the previous Hazelcast releases, WAN replication was dropping the new events if
WAN replication event queues are full. This behavior is configurable.

The following behaviors are supported:

* `DISCARD_AFTER_MUTATION`: If you select this option, the new WAN events generated by
the member are dropped and not replicated to the target cluster
when the WAN event queues are full.
* `THROW_EXCEPTION`: If you select this option, the WAN queue size is checked before
each supported mutating operation (like `IMap.put()`, `ICache.put()`).
If one the queues of target cluster is full, `WanReplicationQueueFullException` is thrown and
the operation is not allowed.
* `THROW_EXCEPTION_ONLY_IF_REPLICATION_ACTIVE`: Its effect is similar to that of `THROW_EXCEPTION`.
But, it  throws exception only when WAN replication is active. It discards the new events if WAN replication is stopped.

The following is an example configuration:

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster">
        <batch-publisher>
            <cluster-name>test-cluster-1</cluster-name>
            <queue-full-behavior>DISCARD_AFTER_MUTATION</queue-full-behavior>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

NOTE: `queue-full-behavior` configuration is optional. Its default value is `DISCARD_AFTER_MUTATION`.

==== Event Filtering API

Starting with Hazelcast 3.6, WAN replication allows you to intercept WAN replication events before they are placed to
WAN event replication queues by providing a filtering API.
Using this API, you can monitor WAN replication events of each data structure
separately.

You can attach filters to your data structures using the `filter` element of
`wan-replication-ref` configuration inside `hazelcast.xml` as shown below.
You can also configure it using the programmatic configuration.

[source,xml]
----
<hazelcast>
    ...
    <map name="testMap">
        <wan-replication-ref name="test">
            <filters>
                <filter-impl>com.example.MyFilter</filter-impl>
                <filter-impl>com.example.MyFilter2</filter-impl>
            </filters>
        </wan-replication-ref>
    </map>
    ...
</hazelcast>
----

As shown in the above configuration, you can define more than one filter. Filters are called in the order that they are introduced.
A WAN replication event is only eligible to publish if it passes all the filters.

Map and Cache have different filter interfaces: `MapWanEventFilter` and
`CacheWanEventFilter`. Both of these interfaces have the method `filter` which takes the following parameters:

* `mapName`/`cacheName`: Name of the related data structure.
* `entryView`: link:https://docs.hazelcast.org/docs/latest/javadoc/com/hazelcast/core/EntryView.html[EntryView^]
or link:https://docs.hazelcast.org/docs/latest/javadoc/com/hazelcast/cache/CacheEntryView.html[CacheEntryView^] depending on the data structure.
* `eventType`: Enum type - `UPDATED(1)`, `REMOVED(2)` or `LOADED(3)` - depending on the event.

NOTE: `LOADED` events are filtered out and not replicated to target cluster.

==== Acknowledgment Types

Starting with Hazelcast 3.6, WAN replication supports different acknowledgment (ACK) types for each target cluster.
You can choose from 2 different ACK type depending on your consistency requirements. The following ACK types are supported:

* `ACK_ON_RECEIPT`: A batch of replication events is considered successful as soon as
it is received by the target cluster. This option does not guarantee that the received event is actually applied but it is faster.
* `ACK_ON_OPERATION_COMPLETE`: This option guarantees that the event is received by
the target cluster and it is applied. It is more time consuming.
But it is the best way if you have strong consistency requirements.

The following is an example configuration:

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster">
        <batch-publisher>
            <cluster-name>test-cluster-1</cluster-name>
            <acknowledge-type>ACK_ON_OPERATION_COMPLETE</acknowledge-type>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

NOTE: `acknowledge-type` configuration is optional. Its default value is `ACK_ON_OPERATION_COMPLETE`.


==== Synchronizing WAN Target Cluster

Starting with Hazelcast 3.7 you can initiate a synchronization operation on an IMap for a specific target cluster.
Synchronization operation sends all the data of an IMap to a target cluster to align the state of target IMap with source IMap.
Synchronization is useful if two remote clusters lost their synchronization due to WAN queue overflow or in restart scenarios.

Synchronization can be initiated through
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#wan-sync[Management Center^] and
Hazelcastâ€™s REST API.

Below is the URL for the REST call;

```
http://member_ip:port/hazelcast/rest/wan/sync/map
```

You need to add URL-encoded parameters to the request in the following order separated by "&";

* Cluster name
* Cluster password
* Name of the WAN replication configuration
* WAN replication publisher ID/target cluster name
* Map name to be synchronized

Assume that you have configured an IMap with a WAN replication configuration as follows:

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster">
        <batch-publisher>
            <cluster-name>istanbul</cluster-name>
        </batch-publisher>
    </wan-replication>
    <map name="my-map">
        <wan-replication-ref name="my-wan-cluster">
            <merge-policy>com.hazelcast.spi.merge.PassThroughMergePolicy</merge-policy>
        </wan-replication-ref>
    </map>
    ...
</hazelcast>
----

Then, an example CURL command to initiate the synchronization for "my-map" would be as follows:

```
curl -X POST -d "clusterName&clusterPassword&my-wan-cluster&istanbul&my-map" --URL http://127.0.0.1:5701/hazelcast/rest/wan/sync/map
```

[NOTE]
.Synchronizing All Maps
====

You can also use the following URL in your REST call if you want to
synchronize all the maps in source and target cluster:

`+http://member_ip:port/hazelcast/rest/wan/sync/allmaps+`

You need to add the following URL-encoded parameters to
the request in the following order separated by "&";

* Cluster name
* Cluster password
* Name of the WAN replication configuration
* WAN replication publisher ID/target cluster name
====

NOTE: Synchronization for a target cluster operates only with
the data residing in the memory. Therefore, evicted entries are not
synchronized, not even if `MapLoader` is configured.

==== Dynamically Adding WAN Publishers

When running clusters for an extensive period, you might need to
dynamically change the configuration while the cluster is running.
This includes dynamically adding new WAN replication publishers (new target clusters) and
replicating the subsequent map and cache updates to the new publishers without any manual intervention.

You can add new WAN publishers to an existing WAN replication using
almost all of the configuration options that are available when
configuring the WAN publishers in the static configuration (including using Discovery SPI).
The new configuration is not persisted but it is replicated to all existing and new members.
Once the cluster is completely restarted, the dynamically added publisher configuration is lost and
the updates are not replicated to the target cluster anymore until added again.

If you wish to preserve the new configuration over cluster restarts, you must add
the exact same configuration to the static configuration file after dynamically adding the publisher configuration to a running cluster.

You cannot remove the existing configurations but can put the publishers into
a STOPPED state which prevents the WAN events from being enqueued in the WAN queues and
prevents the replication, rendering the publisher idle. The configurations also cannot be changed.

Synchronization can be initiated through
link:https://docs.hazelcast.org/docs/management-center/latest/manual/html/index.html#wan-sync[Management Center^] and
Hazelcastâ€™s REST API.

Below is the URL for the REST call:

```
http://member_ip:port/hazelcast/rest/wan/addWanConfig
```

You need to add the following URL-encoded parameters to the request in the following order separated by "&";

* Cluster name
* Cluster password
* WAN replication configuration, serialized as JSON

You can, at any point, even when maps and caches are concurrently mutated, add a new WAN publisher to
an existing WAN replication configuration.
The limitation is that there must be an existing WAN replication configuration but
it can be empty, without any publishers (target clusters).
For instance, this is an example of an XML configuration to which you can dynamically add new publishers:

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="wanReplication"></wan-replication>
    <map name="my-map">
        <wan-replication-ref name="wan-replication">
            <merge-policy>com.hazelcast.spi.merge.PassThroughMergePolicy</merge-policy>
            <republishing-enabled>false</republishing-enabled>
       </wan-replication-ref>
    </map>
    ...
</hazelcast>
----

Note that the map has defined WAN replication but there is no target cluster yet.
You can then add the new WAN replication publishers (target clusters) by
performing an HTTP POST as shown below:

```
curl -X POST -d "clusterName&clusterPassword&{...}" --URL http://127.0.0.1:5701/hazelcast/rest/wan/addWanConfig

```

You can provide the full configuration as JSON as a parameter.
Any WAN configuration supported in the XML and programmatic configurations is also supported in this JSON format.
Below are some examples of JSON configuration for a WAN publisher using
the Discovery SPI and static IP configuration. Here are the integer values for `initialPublisherState`,
`queueFullBehavior` and `consistencyCheckStrategy`:

* `initialPublisherState`:
** 0: REPLICATING
** 1: PAUSED
** 2: STOPPED
* `queueFullBehavior`:
** 0: DISCARD_AFTER_MUTATION
** 1: THROW_EXCEPTION
** 2: THROW_EXCEPTION_ONLY_IF_REPLICATION_ACTIVE
* `consistencyCheckStrategy`:
** 0: NONE
** 1: MERKLE_TREES


Below is an example using Discovery SPI (AWS configuration):

```
{
   "name":"wanReplication",
   "publishers":[
      {
         "clusterName":"tokyo",
         "queueCapacity":10000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "discovery":{
            "nodeFilterClass":null,
            "discoveryStrategy":[
               {
                  "className":"com.hazelcast.aws.AwsDiscoveryStrategy",
                  "properties":{
                     "security-group-name":"hazelcast",
                     "tag-value":"cluster1",
                     "host-header":"ec2.amazonaws.com",
                     "tag-key":"aws-test-cluster",
                     "secret-key":"my-secret-key",
                     "iam-role":"s3access",
                     "access-key":"my-access-key",
                     "hz-port":"5701-5708",
                     "region":"us-west-1"
                  }
               }
            ]
         }
      }
   ]
}
```

Below is an example with Discovery SPI (the new AWS configuration)

```
{
   "name":"wanReplication",
   "publishers":[
      {
         "clusterName":"tokyo",
         "queueCapacity":1000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "aws":{
            "enabled":true,
            "usePublicIp":false,
            "properties":{
               "security-group-name":"hazelcast-sg",
               "tag-value":"hz-nodes",
               "host-header":"ec2.amazonaws.com",
               "tag-key":"type",
               "secret-key":"my-secret-key",
               "iam-role":"dummy",
               "access-key":"my-access-key",
               "region":"us-west-1"
            }
         },
         "sync":{
            "consistencyCheckStrategy":0
         }
      }
   ]
}
```

Below is an example with static IP configuration (with some optional attributes):

```
{
   "name":"wanReplication",
   "publishers":[
      {
         "clusterName":"tokyo",
         "queueCapacity":1000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "responseTimeoutMillis":5000,
         "targetEndpoints":"10.3.5.1:5701, 10.3.5.2:5701",
         "batchMaxDelayMillis":3000,
         "batchSize":50,
         "snapshotEnabled":false,
         "acknowledgeType":1,
         "sync":{
            "consistencyCheckStrategy":0
         }
      }
   ]
}
```

Below is an XML configuration with two publishers and several (disabled) discovery strategy configurations:

```
{
   "name":"wanReplication",
   "publishers":[
      {
         "clusterName":"tokyo",
         "queueCapacity":1000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "aws":{
            "enabled":true,
            "usePublicIp":false,
            "properties":{
               "security-group-name":"hazelcast-sg",
               "tag-value":"hz-nodes",
               "host-header":"ec2.amazonaws.com",
               "tag-key":"type",
               "secret-key":"my-secret-key",
               "iam-role":"dummy",
               "access-key":"my-access-key",
               "region":"us-west-1"
            }
         },
         "gcp":{
            "enabled":false,
            "usePublicIp":true,
            "properties":{
               "gcp-prop":"gcp-val"
            }
         },
         "azure":{
            "enabled":false,
            "usePublicIp":true,
            "properties":{
               "azure-prop":"azure-val"
            }
         },
         "kubernetes":{
            "enabled":false,
            "usePublicIp":true,
            "properties":{
               "k8s-prop":"k8s-val"
            }
         },
         "eureka":{
            "enabled":false,
            "usePublicIp":true,
            "properties":{
               "eureka-prop":"eureka-val"
            }
         },
         "discovery":{
            "nodeFilterClass":null,
            "discoveryStrategy":[

            ]
         },
         "sync":{
            "consistencyCheckStrategy":0
         }
      },
      {
         "clusterName":"london",
         "queueCapacity":1000,
         "queueFullBehavior":0,
         "initialPublisherState":0,
         "responseTimeoutMillis":5000,
         "targetEndpoints":"10.3.5.1:5701, 10.3.5.2:5701",
         "batchMaxDelayMillis":3000,
         "batchSize":50,
         "snapshotEnabled":false,
         "acknowledgeType":1,
         "aws":{
            "enabled":false,
            "usePublicIp":false
         },
         "gcp":{
            "enabled":false,
            "usePublicIp":false
         },
         "azure":{
            "enabled":false,
            "usePublicIp":false
         },
         "kubernetes":{
            "enabled":false,
            "usePublicIp":false
         },
         "eureka":{
            "enabled":false,
            "usePublicIp":false
         },
         "discovery":{
            "nodeFilterClass":null,
            "discoveryStrategy":[

            ]
         },
         "sync":{
            "consistencyCheckStrategy":1
         }
      }
   ]
}
```


==== WAN Replication Failure Detection and Recovery

The failure detection and recovery mechanisms in WAN handle failures during WAN replication and
they closely interact with the list of endpoints that WAN is replicating to.
There might be some small differences when using static endpoints or the discovery SPI but
here we will outline the general mechanism of failure detection and recovery.

===== WAN Target Endpoint List

The WAN connection manager maintains a list of public addresses that it can replicate to at any moment.
This list may change over time as failures are detected or as new addresses are discovered when using the discovery SPI.
The connection manager does not eagerly create connections to these addresses as
they are added to the list to avoid overloading the endpoint with connections from all members using
the same configuration. It tries to connect to the endpoint just before WAN events are about to be transmitted.
This means that if there are no updates on the map or cache using WAN replication, there are no
WAN events and the connection will not be established to the endpoint.

When more than one endpoint is configured, traffic is load balanced between them using
the partition, so that the same partitions are always sent to the same target member, ensuring ordering by partition.

===== WAN Failure Detection

If using the Hazelcast IMDG Enterprise edition class `WanBatchReplication`
(see the <<defining-wan-replication, Defining WAN replication section>>), the WAN replication catches
any exceptions when sending the WAN events to the endpoint. In the case of an exception,
the endpoint is removed from the endpoint list to which WAN replicates and the WAN events are resent to
a different address. The replication is retried until it is successful.

===== WAN Endpoint Recovery

The WAN connection manager tries to "rediscover" new endpoints periodically.
The period is 10 seconds by default but can be configured using the `discovery-period-seconds` element
(see the <<defining-wan-replication, Defining WAN replication section>>).

The discovered endpoints depend on the configuration used to define WAN replication.
If using static WAN endpoints (by using the `target-endpoints` element), the discovered endpoints are always the same and
are equal to the values defined in the configuration.
If using discovery SPI with WAN, the discovered endpoints may be different each time.

When the discovery returns a list of endpoints (addresses), the WAN target endpoint list is updated.
Newly discovered endpoints are added and endpoints which are no longer in the discovered list are removed.
Newly discovered endpoints may include addresses to which WAN replication has previously failed.
This means that once a new WAN event is about to be sent, a connection is reestablished to
the previously failed endpoint and WAN replication is retried. The endpoint can later
be again removed from the target endpoint list if the replication again encounters failure.

[[tune-wr]]
==== Tuning WAN Replication For Lower Latencies and Higher Throughput

Starting with Hazelcast IMDG 3.12, we have redesigned the WAN replication mechanism to allow
tuning for lower latencies of replication and higher throughput.
In most cases, WAN replication is sufficient with out-of-the-box settings which cause
WAN replication to replicate the map and cache events with little overhead.
However, there might be some use cases where the latency between a map/cache mutation on one cluster and
its visibility on the other cluster must be kept within some bounds.
To achieve such demands, you can first try tuning the WAN replication mechanism using the following publisher elements:

* `batch-size`
* `batch-max-delay-millis`
* `idle-min-park-ns`
* `idle-max-park-ns`

To understand the implications of these elements, let's first dive into how WAN replication works.

WAN replication runs in a separate thread and tries to send map and cache mutation events in batches to
the target endpoints for higher throughput. The target endpoints are usually members in
a target Hazelcast cluster but different WAN implementations may have different target endpoints.
The event batch is collected by iterating over the WAN queues for different partitions and, different maps and caches.
WAN replication tries and collects a batch of a size which can be configured using the `batch-size` element.

If enough time has passed and the WAN replication thread hasn't collected enough events to fill
a batch, it sends what it has collected nevertheless.
This is controlled by the `batch-max-delay-millis` element.
The "enough time" precisely means that more than the configured amount of milliseconds has passed since
the time the last batch was sent to any target endpoint.

If there are no events in any of the WAN queues, the WAN replication thread goes into
the idle state by parking the WAN replication thread.
The minimum park time can be defined using the `idle-min-park-ns` element and
the maximum park time can be controlled using the `idle-max-park-ns` element.
If a WAN event is enqueued while the WAN replication thread is in the idle state, the latency for replication of that WAN event increases.

An example WAN replication configuration using the default values of the above elements is shown below.

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster-batch">
        <batch-publisher>
            <cluster-name>london</cluster-name>
            <batch-size>500</batch-size>
            <batch-max-delay-millis>1000</batch-max-delay-millis>
            <idle-min-park-ns>10000000</idle-min-park-ns> <!-- 10 ms -->
            <idle-max-park-ns>250000000</idle-max-park-ns> <!-- 250 ms -->
            ...
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

We will now discuss tuning these elements. Unfortunately, the exact tuning parameters heavily depend
on the load, mutation rate, latency between the source and target clusters and even use cases.
We will thus discuss some general approaches and pointers.

When tuning for low latency, the first thing you might want to do is lower
the `idle-min-park-ns` and `idle-max-park-ns` element values.
This will affect the latencies that you see when having a low number of
operations per second, since this is when the WAN replication thread will be mostly in idle state.
Try lowering both elements but keep in mind that the lower the element value, the more time the WAN replication thread will
spend consuming CPU in a quiescent state - when there is no mutation on the maps or caches.

The next element you might lower is the `batch-max-delay-millis`. If you have a strict upper bound on
the latency for WAN replication, this element must be below that limit. Setting this value too low might
adversely affect the performance as well since then the WAN replication thread might be sending
smaller batches than what it would if the element was higher and it had waited for some more time.
You can even try setting this element to zero which instructs the WAN replication thread to
send batches as soon as it is able to collect any events; but keep in mind this will result in
many smaller batches instead of less bigger event batches.

When tuning for lower latencies, configuring the `batch-size` usually has little effect, especially at lower mutation rates.
At a low number of operations per second,  the event batches will usually be very small since
the WAN replication thread will not be able to collect the full batch and respect the required latencies for replication.
The `batch-size` element might have more effect at higher mutation rates. Here, you will probably want to use
bigger batches to avoid paying for the latencies when sending lots of smaller batches, so try increasing
the batch size and benchmarking at high load.

There are a couple of other configuration values that you might try changing but it depends on your use case.
The first one is adding a separate configuration for a WAN replication executor.
Collecting of WAN event batches and processing the responses from the target endpoints are done on a shared executor.
This executor is shared between the other parts of the Hazelcast system and all of the WAN replication publishers will use
the same executor. In some cases, you might want to create a dedicated executor for all WAN replication publishers.
The name of this executor is `hz:wan`. Below is an example of a concrete, dedicated executor for WAN replication.
See the <<configuring-executor-service, Configuring Executor Service section>> for more information on
the configuration options of the executor.

[source,xml]
----
<hazelcast>
    ...
    <executor-service name="hz:wan">
        <pool-size>16</pool-size>
    </executor-service>
    ...
</hazelcast>
----

The last two elements that you might want to change are `acknowledge-type` and `max-concurrent-invocations`.
Changing these elements allow you to get a greater throughput at the expense of event ordering.
This means that these elements may only be changed if your application can tolerate WAN events to be received out-of-order.
For instance, if you are updating or removing the existing map or cache entries, an out-of-order WAN event delivery would mean
that the event for the entry removal or update might be processed by the target cluster before the event is received to create that entry.
This does not causes exceptions but it causes the clusters to fall out-of-sync.
In these cases, you most probably will not be able to use these elements.
On the other hand, if you are only creating new, immutable entries (which are then removed by the expiration mechanism),
you can use these elements to achieve a greater throughput.

The `acknowledge-type` element controls at which time the target cluster will send a response for the received WAN event batch.
The default value is `ACK_ON_OPERATION_COMPLETE` which will ensure that all events are processed before
the response is sent to the source cluster.
The value `ACK_ON_RECEIPT` instructs the target cluster to send a response as soon as
it has received the WAN event batch but before it has been processed.
This has two implications. One is that events can now be processed out-of-order (see the previous paragraph) and
the other is that the exceptions thrown on processing the WAN event batch will not be received by
the source cluster and the WAN event batch will not be retried.
As such, some events might get lost in case of errors and the clusters may fall out-of-sync.
WAN sync can help bring those clusters in-sync.
The benefit of the `ACK_ON_RECEIPT` value is that now the source cluster can
send a new batch sooner, without waiting for the previous batch to be processed fully.

NOTE: WAN synchronization strategies (neither the default nor the <<delta-wan-synchronization>>)
don't synchronize the deletions since they are not yet tracked under WAN.

The `max-concurrent-invocations` element controls the maximum number of
WAN event batches being sent to the target cluster concurrently.
Setting this element to anything less than 2 will only allow a single batch of
events to be sent to each target endpoint and will maintain causality of events for
a single partition (events are not received out-of-order).
Setting this element to 2 or higher will allow multiple batches of WAN events to be sent to
each target endpoint. Since this allows reordering of batches due to the network conditions, causality and
ordering of events for a single partition is lost and batches for a single partition are now sent randomly to
any available target endpoint. This, however, does present a faster WAN replication for certain scenarios such as
replicating immutable, independent map entries which are only added once and where
ordering, when these entries are added, is not necessary.
Keep in mind that if you set this element to a value which is less than the target endpoint count,
you will lose performance as not all target endpoints will be used at any point in time to process the WAN event batches.
So, for instance, if you have a target cluster with 3 members (target endpoints) and you want to use
this element, it only makes sense to set it to a value higher than 3. Otherwise, you can simply disable it by
setting it to less than 2 in which case WAN will use the default replication strategy and adapt to
the target endpoint count while maintaining causality.

An example WAN replication configuration using the default values of
the aforementioned elements is shown below.

[source,xml]
----
<hazelcast>
    ...
    <wan-replication name="my-wan-cluster-batch">
        <batch-publisher>
            <cluster-name>london</cluster-name>
            <acknowledge-type>ACK_ON_OPERATION_COMPLETE</acknowledge-type>
            <max-concurrent-invocations>-1</max-concurrent-invocations>
            ...
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

Finally, as we've mentioned, the exact values which will give you the optimal performance depend on your environment and use case.
Please benchmark and try out different values to find out the right values for you.

==== WAN Replication Additional Information

Each cluster in WAN topology has to have a unique `cluster-name` property for a proper handling of forwarded events.

Starting with Hazelcast 3.6, WAN replication backs up its event queues to other members to prevent event loss in case of member failures.
WAN replication's backup mechanism depends on the related data structures' backup operations. Note that, WAN replication is supported for IMap and ICache.
That means, as far as you set a backup count for your IMap or ICache instances, WAN replication events generated by these instances are also replicated.

There is no additional configuration to enable/disable WAN replication event backups.

=== Delta WAN Synchronization

Hazelcast clusters connected over WAN can go out-of-sync because of various reasons such as member failures and concurrent updates.
To overcome the out-of-sync issue, Hazelcast has the default
<<synchronizing-wan-target-cluster, WAN synchronization>> feature, through which
the maps in different clusters are synced by transferring all entries from the source to the target cluster.
This may be not efficient since some of the entries have remained unchanged on both clusters and
do not require to be transferred. Also, for the entries to be transferred, they need to be copied to
on-heap on the source cluster. This may cause spikes in the heap usage, especially if using large off-heap stores.

Besides the default WAN synchronization, Hazelcast provides Delta WAN Synchronization which uses
link:https://en.wikipedia.org/wiki/Merkle_tree[Merkle tree^] for the same purpose.
It is a data structure used for efficient comparison of the difference in the contents of large data structures.
The precision of this comparison is defined by Merkle tree's depth.
Merkle tree hash exchanges can detect inconsistencies in the map data and
synchronize only the different entries when using WAN synchronization, instead of sending all the map entries.

NOTE: Currently, Delta WAN Synchronization is implemented only for Hazelcast IMap.
It will also be implemented for ICache in the future releases.

==== Requirements

To be able to use Delta WAN synchronization, the following must be met:

* Source and target cluster versions must be at least Hazelcast 3.11.
* Both clusters must have the same number of partitions.
* Both clusters must use the same partitioning strategy.
* Both clusters must have the Merkle tree structure enabled.

==== Using Delta WAN Synchronization

To be able to use Delta WAN synchronization for a Hazelcast data structure:

. Configure the WAN synchronization mechanism for your WAN publisher so that
it uses the Merkle tree: If configuring declaratively, you can use the `consistency-check-strategy` sub-element of
the `wan-sync` element. If configuring programmatically, you can use the setter of the
link:https://docs.hazelcast.org/docs/latest/javadoc/com/hazelcast/config/WanSyncConfig.html[WanSyncConfig^] object.
. Bind that WAN synchronization configuration to the data structure (currently IMap):
Simply set the WAN replication reference of your map to the name of the WAN replication configuration which uses the Merkle tree.

Following is a declarative configuration example of the above:

[source,xml]
----
<hazelcast>
    ...
    <map name="myMap">
        <wan-replication-ref name="wanReplicationScheme">
        </wan-replication-ref>
    </map>
    <wan-replication name="wanReplicationScheme">
        <batch-publisher>
            <cluster-name>clusterName</cluster-name>
            <wan-sync>
                <consistency-check-strategy>MERKLE_TREES</consistency-check-strategy>
            </wan-sync>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

Here, the element `consistency-check-strategy` sets the strategy for
checking the consistency of data between the source and target clusters.
You must initiate the WAN synchronization (via Management Center or REST API as explained in
<<synchronizing-wan-target-cluster, Synchronizing WAN clusters>>) to let this strategy reconcile the inconsistencies.
The element `consistency-check-strategy` has currently two values:

* `NONE`: Means that there are no consistency checks. This is the default value.
* `MERKLE_TREES`: Means that WAN synchronization uses Merkle tree structure.

==== Configuring Delta WAN Synchronization

You can configure Delta WAN Synchronization declaratively using the `merkle-tree` element or programmatically using the link:https://docs.hazelcast.org/docs/latest/javadoc/com/hazelcast/config/MerkleTreeConfig.html[MerkleTreeConfig^] object.

Following is a declarative configuration example showing how to
enable Delta WAN Synchronization, bind it to a Hazelcast data structure (an IMap in the below case) and specify its depth.

[source,xml]
----
<hazelcast>
    ...
    <map name="someMap">
        <merkle-tree enabled="true">
            <depth>5</depth>
        </merkle-tree>
    </map>
    ...
</hazelcast>
----

Here are the descriptions of sub-elements and attributes:

* `enabled`: Specifies whether the Merkle tree structure is enabled. Its default value is `true`.
* `mapName`: Specifies the name of the map for which the Merkle tree structure is used.
* `depth`: Specifies the depth of Merkle tree. Valid values are between 2 and 27 (exclusive). Its default value is `10`.
** A larger depth means that a data synchronization mechanism is able to pinpoint a smaller subset of
the data structure (e.g., IMap) contents in which a change has occurred.
This causes the synchronization mechanism to be more efficient.
However, keep in mind that a large depth means that the Merkle tree will consume more memory.
As the comparison mechanism is iterative, a larger depth also prolongs the comparison duration.
Therefore, it is recommended not to have large tree depths if the latency of the comparison operation is high.
** A smaller depth means that the Merkle tree is shallower and the data synchronization mechanism transfers
larger chunks of the data structure (e.g., IMap) in which a possible change has happened.
As you can imagine, a shallower Merkle tree will consume less memory.

Following is a declarative example including the Merkle tree configuration.

[source,xml]
----
<hazelcast>
    ...
    <map name="myMap">
        <wan-replication-ref name="wanReplicationScheme">
            ...
        </wan-replication-ref>
        <merkle-tree enabled="true">
            <depth>10</depth>
        </merkle-tree>
    </map>

    <wan-replication name="wanReplicationScheme">
        <batch-publisher>
            <cluster-name>clusterName</cluster-name>
            <wan-sync>
                <consistency-check-strategy>MERKLE_TREES</consistency-check-strategy>
            </wan-sync>
        </batch-publisher>
    </wan-replication>
    ...
</hazelcast>
----

NOTE: If you do not specifically configure the `merkle-tree` in your
Hazelcast configuration, Hazelcast uses the default Merkle tree structure values
(i.e., it is enabled by default and its default depth is 10) when there is a WAN publisher using
the Merkle tree (i.e., `consistency-check-strategy` for a WAN replication configuration is set as
`MERKLE_TREES` and there is a data structure using that WAN replication configuration).

NOTE: Merkle trees are created for each partition holding IMap data.
Therefore, increasing the partition count also
increases the efficiency of the Delta WAN Synchronization.

==== The Process

Synchronizing the maps based on Merkle trees consists of two phases:

1. _Consistency check_: Process of exchanging and comparing the hashes stored in the Merkle tree structures in the
source and target clusters. The check starts with the root node and continues recursively with the children with different
hash codes. Both sides send the children of the nodes that the other side sent, hence the comparison is done by `depth/2`
steps. After this check, the tree leaves holding different entries are identified.
2. _Synchronization_: Process of transferring the entries belong to the leaves identified by the _consistency
check_ from the source to target cluster. On the target cluster the configured merge policy is applied for each entry that
is in both the source and target clusters.

NOTE: If you only need the differences between the clusters, you can trigger the consistency check without performing
synchronization.

==== Memory Consumption

Since Merkle trees are built for each partition and each map, the memory overhead of the trees with high entry count and deep
trees can be significant. The trees are maintained on-heap, therefore - besides the memory consumption - garbage collection could be another
concern. Make sure the configuration is tested with realistic data size before deployed in production.

The table below shows a few examples for what the memory overhead could be.

.Merkle trees memory overhead for a member
|===
|Entries Stored |Partitions Owned |Entries per Leaf |Depth |Memory Overhead

|1M
|271
|7
|10
|57 MB

|1M
|271
|1
|13
|97 MB

|10M
|271
|72
|10
|412 MB

|10M
|271
|9
|13
|453 MB

|10M
|5009
|4
|10
|577 MB

|10M
|5009
|1
|12
|900 MB

|25M
|5009
|10
|10
|1986 MB

|25M
|5009
|1
|13
|2740 MB

|===

==== Defining the Depth

The efficiency of the Delta WAN Synchronization (WAN synchronization based on Merkle trees) is determined by the average number of entries per the tree
leaves that is proportionate to the number of entries in the map. The bigger this average the more entries are getting
synchronized for the same difference. Raising the depth decreases this average at the cost of increasing the memory overhead.

This average can be calculated for a map as `avgEntriesPerLeaf = mapEntryCount / totalLeafCount`, where `totalLeafCount =
partitionCount * 2^depth-1^`. The ideal value is 1, however this may come at significant memory overhead as shown in the
table above.

In order to specify the tree depth, a trade-off between memory consumption and effectiveness might be needed.

Even if the map is huge and the Merkle trees are configured to be relatively shallow, the Merkle tree based synchronization
may be leveraged if only a small subset of the whole map is expected to be synchronized. The table below illustrates the
efficiency of the Merkle tree based synchronization compared to the default synchronization mechanism.


.Efficiency examples
|===
|Map entry count |Depth |Memory consumption |Avg entries / leaf |Difference count |Entries synced |Efficiency

|10M
|11
|685 MB
|2
|5M
|10M
|0%

|10M
|12
|900 MB
|1
|5M
|5M
|100%

|10M
|10
|577 MB
|4
|1M
|4M
|150%

|10M
|8
|497 MB
|16
|10K
|160K
|6150%

|10M
|12
|900 MB
|1
|10K
|10K
|99900%

|===

The `Difference count` column shows the number of the entries different in the source and the target clusters.
This is the minimum number of the entries that need to be synchronized to make the clusters consistent. The `Entries synced`
column shows how many entries are synchronized in the given case, calculated as `Entries synced` = `Difference count`
* `Avg entries / leaf`.

As shown in the last two rows, the Merkle tree based synchronization transfers significantly less entries than what the
default mechanism does even with 8 deep trees. The efficiency with depth 12 is even better but consumes much more memory.

NOTE: The averages in the table are calculated with 5009 partitions.

NOTE: The average entries per leaf number above assumes perfect distribution of the entries amongst the leaves. Since this is
typically not true in real-life scenarios the efficiency can be slightly worse. The statistics section below describes how to
get the actual average for the leaves involved in the synchronization.

==== REST API

NOTE: To be able to use the REST calls related to synchronization, you need to enable the
`WAN` REST endpoint group. See the <<using-the-rest-endpoint-groups, Using the REST Endpoint Groups section>> for details.

The two phases of the Merkle tree based synchronization can be triggered by a REST call, as it can be done with the
default synchronization.

The URL for the consistency check REST call:

```
http://member_ip:port/hazelcast/rest/wan/consistencyCheck/map
```

The URL for the synchronization REST call - the same as it is for the default synchronization:

```
http://member_ip:port/hazelcast/rest/wan/sync/map
```

You need to add URL-encoded parameters to the request in both cases in the following order separated by "&";

* Cluster name
* Cluster password
* Name of the WAN replication configuration
* WAN replication publisher ID/target cluster name
* Map name to be synchronized

NOTE: You can also use the following URL in your REST call if you want to synchronize all the maps in source and target cluster:
`+http://member_ip:port/hazelcast/rest/wan/sync/allmaps+`. You need to add the following URL-encoded parameters to
the request in the following order separated by "&";

* Cluster name
* Cluster password
* Name of the WAN replication configuration
* WAN replication publisher ID/target cluster name

NOTE: Consistency check can be triggered only for one map.

==== Statistics

The consistency check and the synchronization both write statistics into the diagnostics subsystem and send it
 to the Management Center. The following reported fields can be used to reason about the efficiency of the configuration.

Consistency check reports the number of the:

* Merkle tree nodes checked
* Merkle tree nodes found to be different
* and entries needed to be synchronized to make the clusters consistent.

Synchronization reports the:

* duration of the synchronization
* number of the entries synchronized
* average number of the entries per tree leaves in the synchronized leaves.
