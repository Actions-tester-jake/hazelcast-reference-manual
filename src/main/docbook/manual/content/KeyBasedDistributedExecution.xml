<?xml version='1.0' encoding='UTF-8'?>
<sect1 id="KeyBasedDistributedExecution" version='5.0' xmlns='http://docbook.org/ns/docbook'
       xmlns:xlink='http://www.w3.org/1999/xlink'
       xmlns:xi='http://www.w3.org/2001/XInclude'
       xmlns:ns5='http://www.w3.org/2000/svg'
       xmlns:ns4='http://www.w3.org/1998/Math/MathML'
       xmlns:ns3='http://www.w3.org/1999/xhtml'
       xmlns:db='http://docbook.org/ns/docbook'>
    <title>Key based Distributed Executions</title>
    <para>
        Hazelcast has a standard way of finding out which member owns/manages
        each key object. Following operations will be routed to the same member,
        since all of them are operating based on the same key, "key1".
        <programlisting language="java"><![CDATA[Hazelcast.getMap("mapa").put("key1", value);
Hazelcast.getMap("mapb").get("key1");
Hazelcast.getMap("mapc").remove("key1");
// since map names are different, operation will be manipulating 
// different entries, but the operation will take place on the 
// same member since the keys ("key1") are the same

Hazelcast.getLock ("key1").lock();
// lock operation will still execute on the same member of the cluster
// since the key ("key1") is same

Hazelcast.getExecutorService().execute(new DistributedTask(runnable, "key1"));
// distributed execution will execute the 'runnable' on the same member
// since "key1" is passed as the key.

]]></programlisting>

        Let say you have a customers map where customerId
        is the key and the customer object is the value. and customer object
        contains the customer's orders. and let say you want to remove one of
        the orders of a customer and return the number of remaining orders.

        Here is how you would normally do it:
        <programlisting language="java"><![CDATA[public static int removeOrder(long customerId, long orderId) throws Exception {
		IMap<Long, Customer> mapCustomers = Hazelcast.getMap("customers");
		mapCustomers.lock (customerId);
		Customer customer = mapCustomers. get(customerId);
		customer.removeOrder (orderId);
		mapCustomers.put(customerId, customer);
		mapCustomers.unlock(customerId);
		return customer.getOrderCount();
}
]]></programlisting>

        There are couple of things we should consider:
        # There are four distributed operations there.. lock, get, put, unlock.. Can we reduce the number of distributed
        operations?
        # Customer object may not be that big but can we not have to pass that
        object through the wire? Notice that, we are actually passing customer
        object through the wire twice; get and put.

        So instead, why not moving the computation over to the member (JVM)
        where your customer data actually is.

        Here is how you can do this with distributed executor service:
        # Send a
        <literal>Callable</literal>
        task to the member owning the key, clusterId.
        #
        <literal>Callable</literal>
        does the deletion of the order right there and returns with the remaining order count.
        # Upon completion of the
        <literal>Callable</literal>
        task, return the result (remaining
        order count).

        Plus you do not have to wait until the the task complete; since
        distributed executions are asynchronous, you can do other things
        meanwhile.

        here is a sample code:
        <programlisting language="java"><![CDATA[public static int removeOrder(long customerId, long orderId) throws Exception {
	   ExecutorService es = Hazelcast.getExecutorService();
	   FutureTask<Integer> task = new DistributedTask<Integer>(new OrderDeletionTask(customerId, orderId), customerId);
	   es.execute(task);
	   int remainingOrders = task.get();
	   return remainingOrders;
}

public static class OrderDeletionTask implements Callable<Integer>, Serializable {

	   private static final long serialVersionUID = 1L;
	   private long customerId;
	   private long orderId;
	
	   public OrderDeletionTask() {
	   }
	   public OrderDeletionTask(long customerId, long orderId) {
	           super();
	           this.customerId = customerId;
	           this.orderId = orderId;
	   }
	   public Integer call () {
	           IMap<Long, Customer> mapCustomers = Hazelcast.getMap("customers");
	           mapCustomers.lock (customerId);
	           Customer customer = mapCustomers. get(customerId);
	           customer.removeOrder (orderId);
	           mapCustomers.put(customerId, customer);
	           mapCustomers.unlock(customerId);
	           return customer.getOrderCount();
	   }
}
]]></programlisting>

        Benefits of doing the same operation with
        <literal>DistributedTask</literal>
        based on the key are:
        # Only one distributed execution (`es.execute(task)`), instead of four.
        # Less data sent over the wire.
        # Since lock/update/unlock cycle is done locally (local to the customer data), lock duration for the
        <literal>Customer</literal>
        entry is much less so enabling higher concurrency.


    </para>
</sect1>
